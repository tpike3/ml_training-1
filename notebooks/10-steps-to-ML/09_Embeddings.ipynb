{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOChJSNXtC9g"
   },
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ"
   },
   "source": [
    "So far, we've represented text in a bagged one-hot encoded form which is a n-dimensional array where each index corresponds to a token. The value at that index corresponds to the number of times the word appears in the sentence. This method forces us to completely lose the structural information in our inputs. \n",
    "\n",
    "```python\n",
    "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]```\n",
    " \n",
    " We've also represented our input in a one-hot encoded form where each token is represented by an n-dimensional array. T\n",
    " \n",
    " ```python\n",
    "[[0. 0. 0. ... 0. 0. 0.]\n",
    " [0. 0. 1. ... 0. 0. 0.]\n",
    " [0. 0. 0. ... 0. 0. 0.]\n",
    " ...\n",
    " [0. 0. 0. ... 0. 0. 0.]\n",
    " [0. 0. 0. ... 0. 0. 0.]\n",
    " [0. 0. 0. ... 0. 0. 0.]]\n",
    "```\n",
    "\n",
    "his allows us to preserve the structural information but there are two major disadvantages here. If we have a large vocabulary, the representation length for each token will be massive leading to large computes. And though we preserve the structure within the text, the actual representation for each token does not preserve any relationship with respect to other tokens.\n",
    "\n",
    "In this notebook, we're going to learn about embeddings and how they address all the shortcomings of the representation methods we've seen so far.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoMq0eFRvugb"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWro5T5qTJJL"
   },
   "source": [
    "* **Objective:**  Represent tokens in text that capture the intrinsic semantic relationships.\n",
    "* **Advantages:** \n",
    "    * Low-dimensionality while capturing relationships.\n",
    "    * Interpretable token representations\n",
    "* **Disadvantages:** None\n",
    "* **Miscellaneous:** There are lot's of pretrained embeddings to choose from but you can also train your own from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2thIKTLYwzJd"
   },
   "source": [
    "# Learning embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlxioJLqx2Ls"
   },
   "source": [
    "The main idea of embeddings is to have fixed length representations for the tokens in a text regardless of the number of tokens in the vocabulary. So instead of each token representation having the shape [1XV] where V is vocab size, each token now has the shape [1 X D] where D is the embedding size (usually 50, 100, 200, 300). The numbers in the representation will no longer be 0s and 1s but rather floats that represent that token in a D-dimensional latent space. If the embeddings really did capture the relationship between tokens, then we should be able to inspect this latent space and confirm known relationships (we'll do this soon).\n",
    "\n",
    "But how do we learn the embeddings the first place? The intuition behind embeddings is that the definition of a token depends on the token itself but on it's context. There are several different ways of doing this:\n",
    "\n",
    "1. Given the word in the context, predict the target word (CBOW - continuous bag of words).\n",
    "2. Given the target word, predict the context word (skip-gram).\n",
    "3. Given a sequence of words, predict the next word (LM - language modeling).\n",
    "\n",
    "All of these approaches involve create data to train our model on. Every word in a sentence becomes the target word and the context words are determines by a window. In the image below (skip-gram), the window size is 2. We repeat this for every sentence in our corpus and this results in our training data for the unsupervised task. This in an unsupervised learning technique since we don't have official labels for contexts. The idea is that similar target words will appear with similar contexts and we can learn this relationship by repeatedly training our mode with (context, target) pairs.\n",
    "\n",
    "<img src=\"figures/skipgram.png\" width=600>\n",
    "\n",
    "We can learn embeddings using any of these approaches above and some work better than others. You can inspect the learned embeddings but the best way to choose an approach is to empirically validate the performance on a supervised task. We can learn embeddings by creating our models in PyTorch but instead, we're going to use a library that specializes in embeddings and topic modeling called [Gensim](https://radimrehurek.com/gensim/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9wr9S6965DD7",
    "outputId": "e527ab0c-559c-4cf2-bdca-cfc554b723f2"
   },
   "outputs": [],
   "source": [
    "# Let's make sure the libraries are installed\n",
    "#!pip install numpy\n",
    "#!pip install gensim\n",
    "#!pip install matplotlib\n",
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "\n",
    "# Now import the libraries\n",
    "import os\n",
    "from argparse import Namespace\n",
    "import copy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "import nltk#; nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-sx-n9655TJ"
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    data_file=\"data/harrypotter.txt\",\n",
    "    embedding_dim=100,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    skip_gram=1, # 0 = CBOW\n",
    "    negative_sampling=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "irvgngOG5yqk",
    "outputId": "8be95f0e-38fd-456f-df57-245b713a8927"
   },
   "outputs": [],
   "source": [
    "# Split text into sentences\n",
    "tokenizer = nltk.data.load('data/punkt/english.pickle')\n",
    "with open(args.data_file, encoding='cp1252') as fp:\n",
    "    book = fp.read()\n",
    "sentences = tokenizer.tokenize(book)\n",
    "print (len(sentences))\n",
    "print (sentences[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTREFDg47Vrx"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IX7pkCbg7WPK",
    "outputId": "d1789fb1-3952-427d-bb4f-684c71397fff"
   },
   "outputs": [],
   "source": [
    "# Clean sentences\n",
    "sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "print (sentences[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dsmEEVzX5ytO",
    "outputId": "14eaadd8-318f-463b-ae38-fda714014083"
   },
   "outputs": [],
   "source": [
    "# Process sentences for gensim\n",
    "sentences = [sentence.split(\" \") for sentence in sentences]\n",
    "print (sentences[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fa-DbjPW-KC3"
   },
   "source": [
    "When we have large vocabularies to learn embeddings for, things can get complex very quickly. Recall that the backpropagation with softmax updates both the correct and incorrect class weights. This becomes a massive computation for every backwas pass we do so a workaround is to use [negative sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) which only updates the correct class and a few arbitrary incorrect classes (negative_sampling=20). We're able to do this because of the large amount of training data where we'll see the same word as the target class multiple times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wzkUkl-b5ywC",
    "outputId": "6c165235-fc86-42f3-b47d-fad93f903f8f"
   },
   "outputs": [],
   "source": [
    "# Super fast because of optimized C code under the hood\n",
    "model = Word2Vec(sentences=sentences, size=args.embedding_dim, \n",
    "                 window=args.window, min_count=args.min_count, \n",
    "                 sg=args.skip_gram, negative=args.negative_sampling)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "gcMb9HUd7vS_",
    "outputId": "6233793a-61a8-4f21-b83e-327ddb8abea4"
   },
   "outputs": [],
   "source": [
    "# Vector for each word\n",
    "model.wv.get_vector(\"potter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "BozlP5WG70Ak",
    "outputId": "d066a258-f789-43f5-e285-11a2655dbe3d"
   },
   "outputs": [],
   "source": [
    "# Get nearest neighbors (excluding itself)\n",
    "model.wv.most_similar(positive=\"scar\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-wgfMnH68vQ"
   },
   "outputs": [],
   "source": [
    "# Save the weights \n",
    "model.wv.save_word2vec_format('model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1OC2smgw1P5"
   },
   "source": [
    "# Pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHUU_-TJz_7K"
   },
   "source": [
    "We can learn embeddings from scratch using one of the approaches above but we can also leverage pretrained embeddings that have been trained on millions of documents. Popular ones include Word2Vec (skip-gram) or GloVe (global word-word co-occurrence). We can validate that these embeddings captured meaningful semantic relationships by confirming them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJNngmZjglVg"
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "5WZ2bT27gncI",
    "outputId": "528ddc8e-d8e4-4ce9-d376-c31adb2e00e1"
   },
   "outputs": [],
   "source": [
    "# Unzip the file (may take ~3 minutes)\n",
    "zipfile = ZipFile(\"data/glove.6B.zip\",\"r\")\n",
    "zipfile.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NNacKuRNi8_x",
    "outputId": "9416cec2-387a-4025-8c65-a1075119f6ce"
   },
   "outputs": [],
   "source": [
    "# Write embeddings\n",
    "embeddings_file = 'glove.6B.{0}d.txt'.format(args.embedding_dim)\n",
    "zipfile.extract(embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SA5Y1BGbbpPo",
    "outputId": "9c4191ce-84ad-4cf8-cb99-7baa9c379a96"
   },
   "outputs": [],
   "source": [
    "# Save GloVe embeddings to local directory in word2vec format\n",
    "word2vec_output_file = '{0}.word2vec'.format(embeddings_file)\n",
    "glove2word2vec(embeddings_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQp1pIJLiZw3"
   },
   "outputs": [],
   "source": [
    "# Load embeddings (may take a minute)\n",
    "glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "HDpHnC0sbpVE",
    "outputId": "f65be2e7-1b52-4859-f514-c72efe7bc0ab"
   },
   "outputs": [],
   "source": [
    "# (king - man) + woman = ?\n",
    "glove.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "_1TatbmvbpYU",
    "outputId": "6f6955bc-5f35-49f2-81a2-5b2b3e91ff99"
   },
   "outputs": [],
   "source": [
    "# Get nearest neighbors (exlcusing itself)\n",
    "glove.wv.most_similar(positive=\"goku\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMljB5MEbpdb"
   },
   "outputs": [],
   "source": [
    "# Reduce dimensionality for plotting\n",
    "X = glove[glove.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "pca_results = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aok5pRqXqcCf"
   },
   "outputs": [],
   "source": [
    "def plot_embeddings(words, embeddings, pca_results):\n",
    "    for word in words:\n",
    "        index = embeddings.index2word.index(word)\n",
    "        plt.scatter(pca_results[index, 0], pca_results[index, 1])\n",
    "        plt.annotate(word, xy=(pca_results[index, 0], pca_results[index, 1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "NOdLFgTOrrBd",
    "outputId": "60dd126b-565c-42ae-bf2c-ff0d203b2aab"
   },
   "outputs": [],
   "source": [
    "plot_embeddings(words=[\"king\", \"queen\", \"man\", \"woman\"], embeddings=glove, \n",
    "                pca_results=pca_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "6667pZttboaj",
    "outputId": "1408c5c8-0195-4fea-ff11-ca4d8cc68806"
   },
   "outputs": [],
   "source": [
    "# Bias in embeddings\n",
    "glove.most_similar(positive=['woman', 'doctor'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGOkD_DGIq7t"
   },
   "source": [
    "# Using Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tu3Bl-_yw9LA"
   },
   "source": [
    "There are several different ways to use embeddings. \n",
    "\n",
    "1. Use your own trained embeddings (trained on an unsupervised dataset).\n",
    "2. Use pretrained embeddings (GloVe, word2vec, etc.)\n",
    "3. Randomly initialized embeddings.\n",
    "\n",
    "Once you have chosen embeddings, you can choose to freeze them or continue to train them using the supervised data (this could lead to overfitting). In this example, we're going to use GloVe embeddings and freeze them during training. Our task will be to predict an article's category given its title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OlZB0vc1Hbb"
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "11CPv1Z-IrD6",
    "outputId": "7791dc95-e1c6-441d-8d7d-1381576c1554"
   },
   "outputs": [],
   "source": [
    "# Load PyTorch library\n",
    "#!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3daO-e9wpXe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ye64bXPrwpaQ"
   },
   "outputs": [],
   "source": [
    "# Set Numpy and PyTorch seeds\n",
    "def set_seeds(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# Creating directories\n",
    "def create_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m5_jiImZ1NAr",
    "outputId": "3b174e83-9e51-412e-bfab-02d3606be15e"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=True,\n",
    "    shuffle=True,\n",
    "    data_file=\"data/news.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"news\",\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    cutoff=25, # token must appear at least <cutoff> times to be in SequenceVocabulary\n",
    "    num_epochs=5,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    num_filters=100,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=100,\n",
    "    dropout_p=0.1,\n",
    ")\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(seed=args.seed, cuda=args.cuda)\n",
    "\n",
    "# Create save dir\n",
    "create_dirs(args.save_dir)\n",
    "\n",
    "# Expand filepaths\n",
    "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GkNclM3I1XMg"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vk6ucvgp1NDD"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "aTU6ENCu1NIc",
    "outputId": "0e1a8e12-fd6e-4073-f4cb-47491aef5401"
   },
   "outputs": [],
   "source": [
    "# Raw data\n",
    "df = pd.read_csv(args.data_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "xIouG0f71NK4",
    "outputId": "7e1cc683-157f-49f3-c853-ce0cd9410756"
   },
   "outputs": [],
   "source": [
    "# Split by category\n",
    "by_category = collections.defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    by_category[row.category].append(row.to_dict())\n",
    "for category in by_category:\n",
    "    print (\"{0}: {1}\".format(category, len(by_category[category])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZyFpi791NNP"
   },
   "outputs": [],
   "source": [
    "# Create split data\n",
    "final_list = []\n",
    "for _, item_list in sorted(by_category.items()):\n",
    "    if args.shuffle:\n",
    "        np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_size*n)\n",
    "    n_val = int(args.val_size*n)\n",
    "    n_test = int(args.test_size*n)\n",
    "\n",
    "  # Give data point a split attribute\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "\n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "NLrQTVoO1NP4",
    "outputId": "8dd24897-f9c8-465f-dded-706faa84cbd2"
   },
   "outputs": [],
   "source": [
    "# df with split datasets\n",
    "split_df = pd.DataFrame(final_list)\n",
    "split_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "v-HSMGwh1NUu",
    "outputId": "1dc58e32-ae79-4ac1-ae6d-af19d0c22754"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "    \n",
    "split_df.title = split_df.title.apply(preprocess_text)\n",
    "split_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPDWGdlE5vye"
   },
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMR5Y3D95v6P"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "\n",
    "        # Token to index\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        # Index to token\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token[token] for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WKnfFKY95v81",
    "outputId": "1835ff66-6548-424e-bdfc-94b0c89d9896"
   },
   "outputs": [],
   "source": [
    "# Vocabulary instance\n",
    "category_vocab = Vocabulary()\n",
    "for index, row in df.iterrows():\n",
    "    category_vocab.add_token(row.category)\n",
    "print (category_vocab) # __str__\n",
    "print (len(category_vocab)) # __len__\n",
    "index = category_vocab.lookup_token(\"Business\")\n",
    "print (index)\n",
    "print (category_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Th-KJnOi_74q"
   },
   "source": [
    "## Sequence vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxO9-6vhBCSO"
   },
   "source": [
    "Next, we're going to create our Vocabulary classes for the article's title, which is a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ip8ViI2v_7Y2"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2LTR5NjBDCG"
   },
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self.mask_token = mask_token\n",
    "        self.unk_token = unk_token\n",
    "        self.begin_seq_token = begin_seq_token\n",
    "        self.end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self.mask_token)\n",
    "        self.unk_index = self.add_token(self.unk_token)\n",
    "        self.begin_seq_index = self.add_token(self.begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self.end_seq_token)\n",
    "        \n",
    "        # Index to token\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'mask_token': self.mask_token,\n",
    "                         'begin_seq_token': self.begin_seq_token,\n",
    "                         'end_seq_token': self.end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_index)\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the SequenceVocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<SequenceVocabulary(size=%d)>\" % len(self.token_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "DvDPVDbsBDE4",
    "outputId": "bce5fc31-b1d7-41d6-e919-1cc72b584e5b"
   },
   "outputs": [],
   "source": [
    "# Get word counts\n",
    "word_counts = Counter()\n",
    "for title in split_df.title:\n",
    "    for token in title.split(\" \"):\n",
    "        if token not in string.punctuation:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "# Create SequenceVocabulary instance\n",
    "title_vocab = SequenceVocabulary()\n",
    "for word, word_count in word_counts.items():\n",
    "    if word_count >= args.cutoff:\n",
    "        title_vocab.add_token(word)\n",
    "print (title_vocab) # __str__\n",
    "print (len(title_vocab)) # __len__\n",
    "index = title_vocab.lookup_token(\"general\")\n",
    "print (index)\n",
    "print (title_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TuUHvlI6JGh"
   },
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8IsOPHT5v_i"
   },
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    def __init__(self, title_vocab, category_vocab):\n",
    "        self.title_vocab = title_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "\n",
    "    def vectorize(self, title):\n",
    "        indices = [self.title_vocab.lookup_token(token) for token in title.split(\" \")]\n",
    "        indices = [self.title_vocab.begin_seq_index] + indices + \\\n",
    "            [self.title_vocab.end_seq_index]\n",
    "        \n",
    "        # Create vector\n",
    "        title_length = len(indices)\n",
    "        vector = np.zeros(title_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "\n",
    "        return vector\n",
    "    \n",
    "    def unvectorize(self, vector):\n",
    "        tokens = [self.title_vocab.lookup_index(index) for index in vector]\n",
    "        title = \" \".join(token for token in tokens)\n",
    "        return title\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cutoff):\n",
    "        \n",
    "        # Create class vocab\n",
    "        category_vocab = Vocabulary()        \n",
    "        for category in sorted(set(df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "\n",
    "        # Get word counts\n",
    "        word_counts = Counter()\n",
    "        for title in df.title:\n",
    "            for token in title.split(\" \"):\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        # Create title vocab\n",
    "        title_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                title_vocab.add_token(word)\n",
    "        \n",
    "        return cls(title_vocab, category_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab = SequenceVocabulary.from_serializable(contents['title_vocab'])\n",
    "        category_vocab = Vocabulary.from_serializable(contents['category_vocab'])\n",
    "        return cls(title_vocab=title_vocab, category_vocab=category_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'title_vocab': self.title_vocab.to_serializable(),\n",
    "                'category_vocab': self.category_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "p2blo9sT5wCS",
    "outputId": "5f43dd85-25ed-4851-e699-e667a06f5d3f"
   },
   "outputs": [],
   "source": [
    "# Vectorizer instance\n",
    "vectorizer = NewsVectorizer.from_dataframe(split_df, cutoff=args.cutoff)\n",
    "print (vectorizer.title_vocab)\n",
    "print (vectorizer.category_vocab)\n",
    "vectorized_title = vectorizer.vectorize(preprocess_text(\n",
    "    \"Roger Federer wins the Wimbledon tennis tournament.\"))\n",
    "print (np.shape(vectorized_title))\n",
    "print (vectorized_title)\n",
    "print (vectorizer.unvectorize(vectorized_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTpYV1FkHBTi"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PcKZGbYIVA7"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lrssjuxb1NZy"
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "        # Max title length\n",
    "        get_length = lambda title: len(title.split(\" \"))\n",
    "        self.max_seq_length = max(map(get_length, df.title)) + 2 # (<BEGIN> + <END>)\n",
    "\n",
    "        # Data splits\n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights (for imbalances)\n",
    "        class_counts = df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self.vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, df, cutoff):\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, NewsVectorizer.from_dataframe(train_df, cutoff))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, df, vectorizer_filepath):\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer)\n",
    "\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NewsVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self.vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self.target_split = split\n",
    "        self.target_df, self.target_size = self.lookup_dict[split]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Dataset(split={0}, size={1})\".format(\n",
    "            self.target_split, self.target_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.target_df.iloc[index]\n",
    "        title_vector = self.vectorizer.vectorize(row.title)\n",
    "        category_index = self.vectorizer.category_vocab.lookup_token(row.category)\n",
    "        return {'title': title_vector, 'category': category_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    def generate_batches(self, batch_size, collate_fn, shuffle=True, \n",
    "                         drop_last=False, device=\"cpu\"):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size,\n",
    "                                collate_fn=collate_fn, shuffle=shuffle, \n",
    "                                drop_last=drop_last)\n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "e9I-AFKsIP2e",
    "outputId": "44c90229-bff4-40a6-977e-6399c793254f"
   },
   "outputs": [],
   "source": [
    "# Dataset instance\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(df=split_df, \n",
    "                                                       cutoff=args.cutoff)\n",
    "print (dataset) # __str__\n",
    "title_vector = dataset[5]['title'] # __getitem__\n",
    "print (title_vector)\n",
    "print (dataset.vectorizer.unvectorize(title_vector))\n",
    "print (dataset.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxjCMr_DINGk"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "202KnN2lLj61"
   },
   "source": [
    "input → embedding → conv → FC \n",
    "\n",
    "We will be using 1d conv operations ([nn.Conv1D](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv1d)) even though our inputs are words because we are not representing them at a character level. The inputs are of shape $\\in \\mathbb{R}^{NXSXE}$\n",
    "* where:\n",
    "    * N = batchsize\n",
    "    * S = max sentence length \n",
    "    * E = embedding dim at a word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-Xgp0F3INRA"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lfw7csveHApA"
   },
   "outputs": [],
   "source": [
    "class NewsModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, num_input_channels, \n",
    "                 num_channels, hidden_dim, num_classes, dropout_p, \n",
    "                 pretrained_embeddings=None, freeze_embeddings=False,\n",
    "                 padding_idx=0):\n",
    "        super(NewsModel, self).__init__()\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                          num_embeddings=num_embeddings,\n",
    "                                          padding_idx=padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                           num_embeddings=num_embeddings,\n",
    "                                           padding_idx=padding_idx,\n",
    "                                           _weight=pretrained_embeddings)\n",
    "        \n",
    "        # Conv weights\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_channels, \n",
    "                                             kernel_size=f) for f in [2,3,4]])\n",
    "     \n",
    "        # FC weights\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(num_channels*3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        if freeze_embeddings:\n",
    "            self.embeddings.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x_in, channel_first=False, apply_softmax=False):\n",
    "        \n",
    "        # Embed\n",
    "        x_in = self.embeddings(x_in)\n",
    "\n",
    "        # Rearrange input so num_channels is in dim 1 (N, C, L)\n",
    "        if not channel_first:\n",
    "            x_in = x_in.transpose(1, 2)\n",
    "            \n",
    "        # Conv outputs\n",
    "        z1 = self.conv[0](x_in)\n",
    "        z1 = F.max_pool1d(z1, z1.size(2)).squeeze(2)\n",
    "        z2 = self.conv[1](x_in)\n",
    "        z2 = F.max_pool1d(z2, z2.size(2)).squeeze(2)\n",
    "        z3 = self.conv[2](x_in)\n",
    "        z3 = F.max_pool1d(z3, z3.size(2)).squeeze(2)\n",
    "        \n",
    "        # Concat conv outputs\n",
    "        z = torch.cat([z1, z2, z3], 1)\n",
    "\n",
    "        # FC layers\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc1(z)\n",
    "        y_pred = self.fc2(z)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d7_4SiLkKji8"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQAIfKN2HAtN"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwgZASWYHAwB"
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
    "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
    "        self.dataset = dataset\n",
    "        self.class_weights = dataset.class_weights.to(device)\n",
    "        self.model = model.to(device)\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
    "        self.train_state = {\n",
    "            'done_training': False,\n",
    "            'stop_early': False, \n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'early_stopping_criteria': early_stopping_criteria,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': model_state_file}\n",
    "    \n",
    "    def update_train_state(self):\n",
    "\n",
    "        # Verbose\n",
    "        print (\"[EPOCH]: {0} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
    "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
    "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
    "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
    "\n",
    "        # Save one model at least\n",
    "        if self.train_state['epoch_index'] == 0:\n",
    "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "            self.train_state['stop_early'] = False\n",
    "\n",
    "        # Save model if performance improved\n",
    "        elif self.train_state['epoch_index'] >= 1:\n",
    "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
    "\n",
    "            # If loss worsened\n",
    "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
    "                # Update step\n",
    "                self.train_state['early_stopping_step'] += 1\n",
    "\n",
    "            # Loss decreased\n",
    "            else:\n",
    "                # Save the best model\n",
    "                if loss_t < self.train_state['early_stopping_best_val']:\n",
    "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "\n",
    "                # Reset early stopping step\n",
    "                self.train_state['early_stopping_step'] = 0\n",
    "\n",
    "            # Stop early ?\n",
    "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
    "              >= self.train_state['early_stopping_criteria']\n",
    "        return self.train_state\n",
    "  \n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        _, y_pred_indices = y_pred.max(dim=1)\n",
    "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "        return n_correct / len(y_pred_indices) * 100\n",
    "    \n",
    "    def pad_seq(self, seq, length):\n",
    "        vector = np.zeros(length, dtype=np.int64)\n",
    "        vector[:len(seq)] = seq\n",
    "        vector[len(seq):] = self.dataset.vectorizer.title_vocab.mask_index\n",
    "        return vector\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        # Make a deep copy\n",
    "        batch_copy = copy.deepcopy(batch)\n",
    "        processed_batch = {\"title\": [], \"category\": []}\n",
    "        \n",
    "        # Get max sequence length\n",
    "        max_seq_len = max([len(sample[\"title\"]) for sample in batch_copy])\n",
    "        \n",
    "        # Pad\n",
    "        for i, sample in enumerate(batch_copy):\n",
    "            seq = sample[\"title\"]\n",
    "            category = sample[\"category\"]\n",
    "            padded_seq = self.pad_seq(seq, max_seq_len)\n",
    "            processed_batch[\"title\"].append(padded_seq)\n",
    "            processed_batch[\"category\"].append(category)\n",
    "            \n",
    "        # Convert to appropriate tensor types\n",
    "        processed_batch[\"title\"] = torch.LongTensor(\n",
    "            processed_batch[\"title\"])\n",
    "        processed_batch[\"category\"] = torch.LongTensor(\n",
    "            processed_batch[\"category\"])\n",
    "        \n",
    "        return processed_batch    \n",
    "  \n",
    "    def run_train_loop(self):\n",
    "        for epoch_index in range(self.num_epochs):\n",
    "            self.train_state['epoch_index'] = epoch_index\n",
    "      \n",
    "            # Iterate over train dataset\n",
    "\n",
    "            # initialize batch generator, set loss and acc to 0, set train mode on\n",
    "            self.dataset.set_split('train')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # zero the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # compute the output\n",
    "                y_pred = self.model(batch_dict['title'])\n",
    "\n",
    "                # compute the loss\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute gradients using loss\n",
    "                loss.backward()\n",
    "\n",
    "                # use optimizer to take a gradient step\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # compute the accuracy\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['train_loss'].append(running_loss)\n",
    "            self.train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            # Iterate over val dataset\n",
    "\n",
    "            # initialize batch generator, set loss and acc to 0; set eval mode on\n",
    "            self.dataset.set_split('val')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            self.model.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # compute the output\n",
    "                y_pred =  self.model(batch_dict['title'])\n",
    "\n",
    "                # compute the loss\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.to(\"cpu\").item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute the accuracy\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['val_loss'].append(running_loss)\n",
    "            self.train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            self.train_state = self.update_train_state()\n",
    "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
    "            if self.train_state['stop_early']:\n",
    "                break\n",
    "          \n",
    "    def run_test_loop(self):\n",
    "        # initialize batch generator, set loss and acc to 0; set eval mode on\n",
    "        self.dataset.set_split('test')\n",
    "        batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred =  self.model(batch_dict['title'])\n",
    "\n",
    "            # compute the loss\n",
    "            loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        self.train_state['test_loss'] = running_loss\n",
    "        self.train_state['test_acc'] = running_acc\n",
    "    \n",
    "    def plot_performance(self):\n",
    "        # Figure size\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        # Plot Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # Plot Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
    "\n",
    "        # Show plots\n",
    "        plt.show()\n",
    "    \n",
    "    def save_train_state(self):\n",
    "        self.train_state[\"done_training\"] = True\n",
    "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
    "            json.dump(self.train_state, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "LJfKGc9cHA0y",
    "outputId": "ed3bbe45-3c58-4ebe-db4d-8efbf1b326d1"
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(df=split_df, \n",
    "                                                       cutoff=args.cutoff)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_embeddings=len(vectorizer.title_vocab), \n",
    "                  num_input_channels=args.embedding_dim, \n",
    "                  num_channels=args.num_filters, hidden_dim=args.hidden_dim, \n",
    "                  num_classes=len(vectorizer.category_vocab), \n",
    "                  dropout_p=args.dropout_p, pretrained_embeddings=None, \n",
    "                  padding_idx=vectorizer.title_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "wDxPyGIVKoUK",
    "outputId": "774c981e-6728-46cd-e1a6-3a8ee5d29a79"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "V9Ruxpc8WT17",
    "outputId": "63933b69-9e07-45f1-82d2-52eab6db9c9a"
   },
   "outputs": [],
   "source": [
    "# Plot performance\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xRphYPaVWTzY",
    "outputId": "c2267e55-ce0c-4e79-a03f-2daf1e8df80d"
   },
   "outputs": [],
   "source": [
    "# Test performance\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_Zrw9rIWTwU"
   },
   "outputs": [],
   "source": [
    "# Save all results\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsWE23n-U-Cy"
   },
   "source": [
    "## Using GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HH_328MjWoM-"
   },
   "source": [
    "We just used some randomly initialized embeddings and we were able to receive decent performance. Keep in mind that this may not always be the case and we may overfit on other datasets with this approach. We're now going to use pretrained GloVe embeddings to initialize our embeddings. We will train our model on the supervised task and assess the performance by first freezing these embeddings (so they don't change during training) and then not freezing them and allowing them to be trained. \n",
    "\n",
    "```python\n",
    "pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "self.embeddings = nn.Embedding(embedding_dim=embedding_dim, \n",
    "                               num_embeddings=num_embeddings, \n",
    "                               padding_idx=padding_idx, \n",
    "                               _weight=pretrained_embeddings)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1EAWPBEKoaS"
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(embeddings_file):\n",
    "    word_to_idx = {}\n",
    "    embeddings = []\n",
    "\n",
    "    with open(embeddings_file, \"r\") as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \")\n",
    "            word = line[0]\n",
    "            word_to_idx[word] = index\n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "\n",
    "    return word_to_idx, np.stack(embeddings)\n",
    "\n",
    "def make_embeddings_matrix(words):\n",
    "    word_to_idx, glove_embeddings = load_glove_embeddings(embeddings_file)\n",
    "    embedding_dim = glove_embeddings.shape[1]\n",
    "    embeddings = np.zeros((len(words), embedding_dim))\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.zeros(1, embedding_dim)\n",
    "            nn.init.xavier_uniform_(embedding_i)\n",
    "            embeddings[i, :] = embedding_i\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZBLYjtWKoX5"
   },
   "outputs": [],
   "source": [
    "args.use_glove_embeddings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Gj77_BmoHA3s",
    "outputId": "d15e120b-4725-49a2-e5a5-cc9f79026f8e"
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(df=split_df, \n",
    "                                                       cutoff=args.cutoff)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = None\n",
    "if args.use_glove_embeddings:\n",
    "    embeddings_file = 'glove.6B.{0}d.txt'.format(args.embedding_dim)\n",
    "    words = vectorizer.title_vocab.token_to_idx.keys()\n",
    "    embeddings = make_embeddings_matrix(words=words)\n",
    "    print (\"<Embeddings(words={0}, dim={1})>\".format(\n",
    "        np.shape(embeddings)[0], np.shape(embeddings)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "CC4ofLswt-D3",
    "outputId": "9ff430ec-4a60-4572-cc8d-5dc5d0eb13e1"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_embeddings=len(vectorizer.title_vocab), \n",
    "                  num_input_channels=args.embedding_dim, \n",
    "                  num_channels=args.num_filters, hidden_dim=args.hidden_dim, \n",
    "                  num_classes=len(vectorizer.category_vocab), \n",
    "                  dropout_p=args.dropout_p, pretrained_embeddings=embeddings, \n",
    "                  padding_idx=vectorizer.title_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "44gVZbKbHA7R",
    "outputId": "18b4c379-3ff5-476d-a297-6ae023d33279"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "98MqlEQ0sfTs",
    "outputId": "3aee1562-ad04-4936-ef1b-31161d8e31c9"
   },
   "outputs": [],
   "source": [
    "# Plot performance\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wYGUHK0GsfhP",
    "outputId": "95fd16d0-5642-4e4c-a617-f4cb41cfcb58"
   },
   "outputs": [],
   "source": [
    "# Test performance\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GO8v_uUisfd9"
   },
   "outputs": [],
   "source": [
    "# Save all results\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbQdvtK3twoW"
   },
   "source": [
    "## Freeze embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrMm4K61t21h"
   },
   "source": [
    "Now we're going to freeze our GloVe embeddings and train on the supervised task. The only modification in the model is to turn on `freeze_embeddings`:\n",
    "\n",
    "```python\n",
    "if freeze_embeddings:\n",
    "    self.embeddings.weight.requires_grad = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilV_QbtktzH3"
   },
   "outputs": [],
   "source": [
    "args.freeze_embeddings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7NdD-iP6tzFQ",
    "outputId": "78702aec-ca66-422f-c412-71f0003cf22f"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_embeddings=len(vectorizer.title_vocab), \n",
    "                  num_input_channels=args.embedding_dim, \n",
    "                  num_channels=args.num_filters, hidden_dim=args.hidden_dim, \n",
    "                  num_classes=len(vectorizer.category_vocab), \n",
    "                  dropout_p=args.dropout_p, pretrained_embeddings=embeddings,\n",
    "                  freeze_embeddings=args.freeze_embeddings,\n",
    "                  padding_idx=vectorizer.title_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "6NJhm2EOtzCV",
    "outputId": "047c1aa8-a44a-4010-c824-a89e30f4157c"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "oojDHLowty_9",
    "outputId": "72dae3b4-af2b-4b53-e5dc-9f6cc92cebac"
   },
   "outputs": [],
   "source": [
    "# Plot performance\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rFYs9dH3ui66",
    "outputId": "0621602f-40c3-4d55-cf6a-a81c503509fe"
   },
   "outputs": [],
   "source": [
    "# Test performance\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vKDmGXAyukpV"
   },
   "outputs": [],
   "source": [
    "# Save all results\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4yiDxOavmJy"
   },
   "source": [
    "So you can see that using GloVe embeddings but not freezing them resulted in the best results on the test dataset. Different tasks will yield different results so you need to choose whether or not to freeze your embeddings based on empirical results."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "12_Embeddings",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
